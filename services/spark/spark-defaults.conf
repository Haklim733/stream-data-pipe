
spark.connect.grpc.binding.port 15002
spark.connect.grpc.binding.host 0.0.0.0
spark.connect.grpc.arrow.enabled true

# --- Master URL Configuration ---
spark.master spark://spark-master:7077

# --- Logging Configuration ---
# Points to log4j2 properties for driver/executor logging.
spark.driver.extraJavaOptions -Dlog4j.configurationFile=file:///opt/spark/conf/log4j2.properties
spark.executor.extraJavaOptions -Dlog4j.configurationFile=file:///opt/spark/conf/log4j2.properties

# --- Serialization Settings ---
# JavaSerializer is generally less efficient than Kryo for large data,
# but can be more compatible. If you hit performance issues, consider Kryo.
spark.serializer org.apache.spark.serializer.JavaSerializer
spark.kryo.registrationRequired false

# --- Network Settings ---
# These values seem reasonable for stability.
spark.network.timeout 800s
spark.executor.heartbeatInterval 60s

# --- Arrow Settings ---
# Good to enable for PySpark performance.
spark.sql.execution.arrow.pyspark.enabled true
spark.sql.execution.arrow.pyspark.fallback.enabled true

# --- I. Resource Management (Memory and CPU) ---
# Your calculations seem reasonable for 64GB system (21GB for Spark cluster, 3 containers).
# Dynamic allocation will try to scale executors between min/max.
spark.executor.memory 2g
spark.executor.cores 1
spark.driver.memory 2g

# Remove spark.cores.max as it can conflict with executor cores
# spark.cores.max 2

spark.dynamicAllocation.enabled true
spark.dynamicAllocation.minExecutors 1
spark.dynamicAllocation.maxExecutors 2
spark.dynamicAllocation.initialExecutors 1

spark.shuffle.service.enabled true
spark.shuffle.compress true
spark.shuffle.registration.timeout 5000
spark.shuffle.manager org.apache.spark.shuffle.sort.SortShuffleManager

# --- III. Network and I/O (S3/MinIO Specific) ---
# These are Hadoop S3A settings. They are very important and look reasonable.
spark.hadoop.fs.s3a.connection.timeout 50000
spark.hadoop.fs.s3a.socket.timeout 50000
spark.hadoop.fs.s3a.max.connections 100
spark.hadoop.fs.s3a.fast.upload.buffer disk
spark.hadoop.fs.s3a.fast.upload.active.blocks 4
spark.hadoop.fs.s3a.multipart.size 104857600
spark.hadoop.fs.s3a.threads.max 20
spark.hadoop.fs.s3a.threads.core 10
spark.hadoop.fs.s3a.buffer.dir /tmp
spark.hadoop.fs.s3a.metrics.enabled true
spark.hadoop.fs.s3a.metrics.reporting.interval 60

# --- IV. SQL Engine Defaults ---
# Adaptive Query Execution (AQE) is generally good.
# spark.sql.shuffle.partitions: 4 is very low for production. For a test/dev cluster, it might be okay.
# For production, aim for 200-2000 depending on data size.
spark.sql.adaptive.enabled true
spark.sql.shuffle.partitions 4
spark.sql.files.maxPartitionBytes 67108864
spark.sql.autoBroadcastJoinThreshold 5242880

# --- V. Memory Management ---
# Standard Spark memory management settings.
spark.memory.fraction 0.6
spark.memory.storageFraction 0.5

# --- CRITICAL: Iceberg REST Catalog Configuration ---
spark.sql.extensions org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.iceberg org.apache.iceberg.spark.SparkCatalog
# spark.sql.catalog.iceberg org.apache.iceberg.rest.RESTCatalog
spark.sql.catalog.iceberg.io-impl org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.iceberg.type rest 
spark.sql.catalog.iceberg.uri http://iceberg-rest:8181
spark.sql.catalog.iceberg.warehouse s3a://iceberg/wh
spark.sql.catalog.iceberg.s3.endpoint http://minio:9000
spark.sql.catalog.iceberg.s3.access-key admin
spark.sql.catalog.iceberg.s3.secret-key password
spark.sql.catalog.iceberg.s3.region us-east-1
spark.sql.catalog.iceberg.s3.path-style-access true
spark.sql.catalog.iceberg.s3.ssl-enabled false

# --- S3A Filesystem configuration ---
spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
spark.hadoop.fs.s3a.endpoint http://minio:9000
spark.hadoop.fs.s3a.path.style.access true
spark.hadoop.fs.s3a.access.key admin
spark.hadoop.fs.s3a.secret.key password 

# Logging configuration (this comment indicates it's moved to session.py - keep it that way if you prefer per-app control)
spark.sql.streaming.checkpointLocation file:///opt/bitnami/spark/logs/checkpoints

# --- Event log settings ---
# Good to have for debugging.
spark.eventLog.enabled true
spark.eventLog.dir file:///opt/bitnami/spark/logs

# --- Adaptive Query Execution ---
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.adaptive.skewJoin.enabled true
spark.sql.adaptive.advisoryPartitionSizeInBytes 128m